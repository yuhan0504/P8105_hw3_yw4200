---
title: "P8105_hw3_yw4200"
author: "yh"
date: "2023-10-05"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1
This problem uses the Instacart data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets using:
```{r}
library(p8105.datasets)
data("instacart")
summary(instacart)
head(instacart,10)
```

The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illstrative examples of observations. 

Answer: The dataset describes some order information and product information about instcart. The dataset has `r nrow(instacart)` observations and `r ncol(instacart)` variables. The key variables include `order_id`, `product_id`, `add_to_cart_order`, `reordered`, `user_id`, `eval_set` etc. Considering about the illstrative examples of observations, i think that each observation is arranged in a confusing order.

Then, do or answer the following (commenting on the results of each):

How many aisles are there, and which aisles are the most items ordered from?
```{r}
# count number of aisles
unique_aisles <- unique(instacart$aisle)
num_aisles <- length(unique_aisles)
num_aisles

# count number of different kinds of aisles
library(dplyr)
aisle_group <- instacart |>
  group_by(aisle) |>
  summarize(total_items = n()) |>
  arrange(desc(total_items))

# select aisle which the most items ordered from
most_items <- aisle_group |>
  filter(aisle_group$total_items == max(aisle_group$total_items))
most_items

```
Answer: There are `r num_aisles` aisles. Fresh vegetables are the most items ordered from.

Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.
```{r}
# select aisles with more than 10000 items ordered
aisle_filtergroup <- aisle_group |>
  filter(total_items > 10000)

# Make a plot that shows the number of items ordered in each aisle
library(ggplot2)
items_number_in_aisles <-
  ggplot(aisle_filtergroup, aes(x = reorder(aisle,-total_items), y = total_items)) +
  geom_bar(stat = "identity", fill = "seagreen3") +
  labs(
    title = "Number of items ordered in each aisle",
    x = "Aisle",
    y = "Total items"
  ) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1)
  ) +
  geom_text(aes(label = total_items), vjust = -1, size = 1.3, color = "black")

items_number_in_aisles
ggsave("The number of items in each aisle.png",plot = items_number_in_aisles)
```

Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.
```{r}
# count the number of items and arrange 
popular_items <- instacart |> 
  filter(aisle %in% c("baking ingredients" , "dog food care", "packaged vegetables fruits") )  |>
  group_by(product_name,aisle) |>
  summarise(items_number = n(),.groups = "drop_last") |>
  arrange(aisle,desc(items_number))

# select the three most popular items
top_items <- popular_items %>%
  group_by(aisle) %>%
  mutate(rank = rank(desc(items_number))) %>%
  filter(rank <= 3)

top_items
```

Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).


```{r}
library(tidyr)
# first filter the Pink Lady Apples and Coffee Ice Cream and rearrange
table_mean_hour <- instacart |>
  filter(product_name %in% c("Pink Lady Apples" , "Coffee Ice Cream")) |>
  group_by(product_name,order_dow) |>
  summarise(mean_hour = mean(order_hour_of_day),.groups = "drop_last") 

# change the value of order_dow
table_mean_hour <- table_mean_hour %>%
  mutate(
    order_dow = case_when(
      order_dow == 0 ~ "Monday",
      order_dow == 1 ~ "Tuesday",
      order_dow == 2 ~ "Wednesday",
      order_dow == 3 ~ "Thursday",
      order_dow == 4 ~ "Friday",
      order_dow == 5 ~ "Saturday",
      order_dow == 6 ~ "Sunday"
    )
  )

# change the table more readable
pivot_table <- table_mean_hour|>
  pivot_wider(names_from = order_dow, values_from = mean_hour) |>
  mutate(across(everything(), ~round(., 2)))
pivot_table
```

## Problem 2
This problem uses the BRFSS data. DO NOT include this dataset in your local data directory; instead, load the data from the p8105.datasets package.
```{r}
data("brfss_smart2010")
unique(brfss_smart2010$Response)
```

First, do some data cleaning:
format the data to use appropriate variable names;
focus on the “Overall Health” topic;
include only responses from “Excellent” to “Poor”;
organize responses as a factor taking levels ordered from “Poor” to “Excellent”
```{r}
brfss_clean <- brfss_smart2010 |>
  janitor::clean_names() |>
  rename(loc_abbreviation = locationabbr,loc_desc = locationdesc) |>
  filter(topic == "Overall Health") |>
  filter(response %in% c("Excellent","Very good","Good","Fair","Poor")) |>
  mutate(response = factor(response, levels = c("Poor", "Fair", "Good", "Very good", "Excellent"),ordered = TRUE))
```

Using this dataset, do or answer the following (commenting on the results of each):

In 2002, which states were observed at 7 or more locations? What about in 2010?
```{r}
# states were observed at 7 or more locations in 2002
brfss_clean_2002 <- brfss_clean |>
  filter(year == 2002) |>
  group_by(loc_abbreviation) |>
  summarise(num_location = n_distinct(loc_desc)) |>
  filter(num_location >= 7) |>
  arrange(desc(num_location))

brfss_clean_2002

# states were observed at 7 or more locations in 2010
brfss_clean_2010 <- brfss_clean |>
  filter(year == 2010) |>
  group_by(loc_abbreviation) |>
  summarise(num_location = n_distinct(loc_desc)) |>
  filter(num_location >= 7) |>
  arrange(desc(num_location))

brfss_clean_2010
```
`r brfss_clean_2002$loc_abbreviation` were observed at 7 or more locations in 2002;`r brfss_clean_2010$loc_abbreviation` were observed at 7 or more locations in 2010.

Construct a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

```{r}
# Construct the dataset
brfss_filter <- brfss_clean |>
  filter(response == "Excellent") |> 
  rename(state = loc_abbreviation) |>
  group_by(year,state) |>
  summarise(avg_data_value = mean(data_value, na.rm = TRUE))
brfss_filter

# Make a “spaghetti” plot
ggplot(brfss_filter, aes(x = year, y = avg_data_value, group = state, color = state)) +
  geom_line() +
  labs(
  title = "Average data value over time within a state",
  x = "Year",
  y = "Average Data Value"
  )
```

Make a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.
```{r}
data_twopanel <- brfss_clean |>
  filter(year == 2006 | year == 2010 & loc_abbreviation == "NY") 

# Create a two-panel plot
two_panel_plot <- ggplot(data_twopanel, aes(x = response, y = data_value)) +
  geom_violin(aes(fill = response),color = "black",alpha = 0.8) +
  facet_wrap(~ year, ncol = 2) +
  labs(title = "Distribution of Data Value in NY State",
       x = "Response",
       y = "Data Value")  +
  theme_minimal()

two_panel_plot
ggsave("Distribution of Data Value in NY State.png", plot = two_panel_plot)
```


```{r}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
